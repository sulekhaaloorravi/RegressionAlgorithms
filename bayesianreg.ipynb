{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa84dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc0562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearRegressionT:\n",
    "    def __init__(self, noise_var, nu=3):\n",
    "        \"\"\"\n",
    "        Bayesian Linear Regression with Student's t-distribution prior for robustness against outliers.\n",
    "        :param noise_var: Observation noise variance (sigma_y^2)\n",
    "        :param nu: Degrees of freedom for the t-distribution (lower values give heavier tails)\n",
    "        \"\"\"\n",
    "        self.noise_var = noise_var  # Noise variance\n",
    "        self.nu = nu  # Degrees of freedom for t-distribution\n",
    "        self.coefficients = None  # Placeholder for the coefficients\n",
    "        self.X_train = None  # To store training data for kernel-based prediction\n",
    "    \n",
    "    def _t_log_prior(self, theta):\n",
    "        \"\"\"\n",
    "        Log-prior for Student's t-distribution.\n",
    "        :param theta: Coefficients\n",
    "        \"\"\"\n",
    "        # Compute the log-pdf of the t-distribution for each coefficient\n",
    "        return np.sum(t.logpdf(theta, df=self.nu))\n",
    "    \n",
    "    def _log_likelihood(self, K, y, theta):\n",
    "        \"\"\"\n",
    "        Log-likelihood for the data given the parameters (Gaussian likelihood).\n",
    "        :param K: Kernel matrix (n_samples, n_samples)\n",
    "        :param y: Target vector\n",
    "        :param theta: Coefficients\n",
    "        \"\"\"\n",
    "        y_pred = np.dot(K, theta)\n",
    "        residual = y - y_pred\n",
    "        log_likelihood = -0.5 * np.sum((residual ** 2) / self.noise_var)\n",
    "        return log_likelihood\n",
    "    \n",
    "    def _log_posterior(self, K, y, theta):\n",
    "        \"\"\"\n",
    "        Log-posterior is a combination of log-prior and log-likelihood.\n",
    "        :param K: Kernel matrix (n_samples, n_samples)\n",
    "        :param y: Target vector\n",
    "        :param theta: Coefficients\n",
    "        \"\"\"\n",
    "        return self._log_likelihood(K, y, theta) + self._t_log_prior(theta)\n",
    "    \n",
    "    def fit(self, X, y, kernel_func, max_iter=1000, lr=0.01):\n",
    "        \"\"\"\n",
    "        Fit the model using kernel transformation and gradient ascent.\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :param kernel_func: Kernel transformation function (e.g., rbf_kernel, polynomial_kernel)\n",
    "        :param max_iter: Maximum iterations for optimization\n",
    "        :param lr: Learning rate for gradient ascent\n",
    "        \"\"\"\n",
    "        self.X_train = X  # Store training data to compute kernel at prediction time\n",
    "        \n",
    "        # Apply kernel transformation to training data\n",
    "        K_train = kernel_func(X)\n",
    "        \n",
    "        n_samples = K_train.shape[0]\n",
    "        \n",
    "        # Initialize coefficients randomly\n",
    "        self.coefficients = np.random.randn(n_samples)\n",
    "        \n",
    "        # Gradient ascent to maximize the log-posterior\n",
    "        for _ in range(max_iter):\n",
    "            grad_log_post = self._compute_gradient(K_train, y, self.coefficients)\n",
    "            self.coefficients += lr * grad_log_post\n",
    "    \n",
    "    def _compute_gradient(self, K, y, theta):\n",
    "        \"\"\"\n",
    "        Numerical gradient of the log-posterior with respect to the coefficients.\n",
    "        :param K: Kernel matrix (n_samples, n_samples)\n",
    "        :param y: Target vector\n",
    "        :param theta: Coefficients\n",
    "        \"\"\"\n",
    "        eps = 1e-5\n",
    "        grad = np.zeros_like(theta)\n",
    "        \n",
    "        for i in range(len(theta)):\n",
    "            theta_pos = np.copy(theta)\n",
    "            theta_neg = np.copy(theta)\n",
    "            theta_pos[i] += eps\n",
    "            theta_neg[i] -= eps\n",
    "            \n",
    "            log_post_pos = self._log_posterior(K, y, theta_pos)\n",
    "            log_post_neg = self._log_posterior(K, y, theta_neg)\n",
    "            \n",
    "            grad[i] = (log_post_pos - log_post_neg) / (2 * eps)\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def predict(self, X_new, kernel_func):\n",
    "        \"\"\"\n",
    "        Predict using the kernel between new data and training data.\n",
    "        :param X_new: New data for prediction\n",
    "        :param kernel_func: Kernel transformation function\n",
    "        \"\"\"\n",
    "        # Compute the kernel between the new data and training data\n",
    "        K_new = kernel_func(X_new, self.X_train)\n",
    "        \n",
    "        # Predict using the kernel-transformed data and learned coefficients\n",
    "        return np.dot(K_new, self.coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714ed5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(X, X_train=None, degree=3, coef0=1):\n",
    "    \"\"\"\n",
    "    Polynomial kernel transformation for non-linear relationships.\n",
    "    :param X: New data\n",
    "    :param X_train: Training data (None if fitting the model, used for prediction)\n",
    "    :param degree: Degree of the polynomial kernel\n",
    "    :param coef0: Bias term\n",
    "    \"\"\"\n",
    "    if X_train is None:\n",
    "        X_train = X\n",
    "    return (np.dot(X, X_train.T) + coef0) ** degree\n",
    "\n",
    "def rbf_kernel(X, X_train=None, gamma=0.1):\n",
    "    \"\"\"\n",
    "    RBF kernel transformation for non-linear relationships.\n",
    "    :param X: New data\n",
    "    :param X_train: Training data (None if fitting the model, used for prediction)\n",
    "    :param gamma: Kernel coefficient\n",
    "    \"\"\"\n",
    "    if X_train is None:\n",
    "        X_train = X\n",
    "    \n",
    "    pairwise_sq_dists = np.sum(X ** 2, axis=1).reshape(-1, 1) + np.sum(X_train ** 2, axis=1) - 2 * np.dot(X, X_train.T)\n",
    "    return np.exp(-gamma * pairwise_sq_dists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cbf6e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with Polynomial Kernel and Student's t Prior: [-4.42350856e+13  1.04970950e+14 -2.75334738e+13 -1.93868590e+13\n",
      " -1.86372537e+13  4.49501731e+13  1.41116618e+13  1.09022023e+13\n",
      "  4.42650142e+12  4.25831885e+14]\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "n_samples, n_features = 100, 4\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_coeff = np.array([4.0, 3.0, 2.0, -1.5])\n",
    "y = np.dot(X, true_coeff) + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Initialize Bayesian regression model with Student's t prior and polynomial kernel\n",
    "bayes_model = BayesianLinearRegressionT(noise_var=0.25, nu=3)\n",
    "\n",
    "# Apply polynomial kernel transformation\n",
    "bayes_model.fit(X, y, kernel_func=polynomial_kernel, max_iter=1000, lr=0.01)\n",
    "\n",
    "# Generate new data for prediction\n",
    "X_new = np.random.randn(10, n_features)\n",
    "\n",
    "# Predict on the new kernel-transformed data\n",
    "predictions = bayes_model.predict(X_new, kernel_func=polynomial_kernel)\n",
    "\n",
    "print(\"Predictions with Polynomial Kernel and Student's t Prior:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20856a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.42350856e+13,  1.04970950e+14, -2.75334738e+13, -1.93868590e+13,\n",
       "       -1.86372537e+13,  4.49501731e+13,  1.41116618e+13,  1.09022023e+13,\n",
       "        4.42650142e+12,  4.25831885e+14])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
